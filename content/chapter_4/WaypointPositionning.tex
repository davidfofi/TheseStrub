\chapter{Waypoints positioning} 
\minitoc
To remind, the main objective is to propose a efficient path to can cover an area using a camera mounted on a UAV. In the solution proposed here is to in the first time, focus on optimize the position of a cameras set to fully cover an area. When a optimized position of the cameras set is found the position can be used as waypoints for an UAVs path. Indeed find an optimized position for each camera of a given set is primordial. The following section are dedicated to the optimization of it in order to have an efficient solution to place the waypoints.




%\section{An optimization problem.}

During the previous section the problem was already discussed as an optimization problem. The formulation of the problem was presented to can apply some optimization problems and also the complexity of the problem was disused in the section \ref{sec:OptimizationComplexity}.



\section{PSO and Random Selection }

The PSO (Particle Swarm Optimization) is an algorithm dedicated to the optimization problems. It is an stochastic algorithms form the family of evolutionary algorithms (see \ref{chap:EA}). 
The PSO is a relatively young compared then the other EA. It was developed by Russel Eberhart and James Kennedy in 1995 [148* bis]. The concept of PSO is to optimize iteratively a continuous non linear function. To do that the PSO is inspired by the behaviour of animals. As it append here form the bird flocking,fish schooling and swarming theory. These animals working in group to find food. 
The direction to take is not decide by one leader, but by all individuals of the swarm by relaying just few informations as what quantities of food their found. 
The swarm composed by numerous individuals became smarter and more efficient to reach their objective. 
The algorithm proposed by Russel Eberhart and James Kennedy in [148* bis] are directly inspired by these behaviours.

The methodologies used is to consider each individual or also called particles as a solution of the problems. The problems is optimized at each iteration. To do that each solution must be comparable and quantifiable. At each iteration, each particle have to be tested by a cost function in order to discriminate the best particles of the swarm. The cost function and the design of it has been detailed in the section \ref{chap:formulation}.
When the best particle is found at the end of an iteration, the other particles of set try to change their initial direction to converge to the best. 
Indeed the power of this algorithm is to have a very basic individuals behaviour. 
Each particles are guided by 3 behaviours.
 \begin{itemize}
 \item  This own velocity $V_k$. 
 \item  This own best solution $P_i$.
 \item  The best solution $P_g$.
\end{itemize}  

To have :
\begin{equation} \label{eq:PSO}
\begin{split}
 V_{k+1}= \omega V_k +b1(P_i -X_k)+b2(P_g-X_k)
\\
\mbox{ and } \\ X_{k+1}=X_k+V_{k+1}
\end{split}
\end{equation}

Where $\omega$ is the inertia. $b1$ is random value between 0 and $\phi_p$ and b2 is random value between 0 and $\phi_g$. $\phi_g$ and $\phi_p$  are the scaling factor to search away from the particleâ€™s best known position (Default: 0.5). 

Thanks to this basic behaviour of the particles the swarm can coverage to an global solution. 
To have an efficient  solution  just few parameters must be set-up.  
The more important are the inertia of the particles and the size of the swarm.\\
The inertia  will globally  help the particles to keep  their  initial velocity. The consequences of the high value of inertia is to explore more the search space and therefore the convergence will be longer. \\
The size of the swarm  have an impact on the  convergence time (in number of iteration) and the also time computation. Indeed a big amount of particles in the swarm  mean more exploration of the search space at each iteration but also more comparison to find the best particles (the comparison may have a non negligible computation time). 
The swarm size is commonly fixed but can be as the population in the GA ( see section \ref{sec:Population} ) dynamically adjusted during the optimization process. 
Other criteria as  $\phi_g$ and $\phi_i$ are minor but can be useful to fine adjust the PSO.

Finally to summarize the PSO is efficient in term of optimization despite a very basic behaviour of each particles. Each particles have this own velocity defined part way by the random and controlled by a global parameters the inertia.

 The power of PSO is at same time this efficiency to solve the optimization problem and this simplicity of use. In fact the PSO need at minima only few element to work properly : a cost function, an inertia parameters and the size of the swarm.





-the initial dispersion of the particles \\
- size swarm \\
- stoping criteria \\
- inconvenient 


148 origine de PSO. 
PSO[84 8 33 143] 87 193* 194* 200* 201* 

228* 161* 158* 78 GA VS PSO

\section{GA VS PSO }\label{sec:GAvsPSO} 
fill with the jirs 
	\subsection{DoF Design of Experiment}
	\subsection{Result}
	\subsection{Explication}

\section{Hybrid GA PSO}
 76* 77* 78*
	\subsection{memetic ???}
		\subsubsection{DoF}
		\subsubsection{Result}
		\subsubsection{Conclusion}
		
\section{Experiment}
	\subsection{No obstacle }
	\subsection{Rectangle obstacle}
	\subsubsection{With mask}
	\subsection{For big area}
		\subsubsection{map 1}
		\subsubsection{map2 torcy}
		\subsubsection{map3 calvisson}
		


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 


